{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdfsampler\n",
    "import natjoin\n",
    "import minhash\n",
    "import random\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(lower, upper, value):\n",
    "    return (float(value) - float(lower)) / (float(upper) - float(lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashsum_key(tables):\n",
    "    for table in tables:\n",
    "        for entry in table:\n",
    "            if \"hash sum\" in entry.keys():\n",
    "                del entry[\"hash sum\"]\n",
    "                \n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdfjoin(tables, sampling_threshold, random_threshold=0.0, timer=0):\n",
    "    if len(tables) <= 1:\n",
    "        return tables\n",
    "    else:\n",
    "        attrs = {}\n",
    "        join_attrs = []\n",
    "        \n",
    "        #find join attributes\n",
    "        for i in range(len(tables)):\n",
    "            table = tables[i]\n",
    "            for table_key in table[0].keys():\n",
    "                if table_key in attrs.keys(): attrs[table_key].append(i)\n",
    "                else: attrs[table_key] = [i]\n",
    "        join_attrs = [k for k,v in attrs.items() if len(v) > 1]\n",
    "        \n",
    "        #generate hash functions for each join_attr\n",
    "        hash_functions = minhash._generate_hash_fns(len(join_attrs))\n",
    "        attrs_hash_dict = {join_attrs[i]: hash_functions[i] for i in range(len(hash_functions))}\n",
    "        \n",
    "        #copy tables\n",
    "        tables_copy = []\n",
    "        for table in tables:\n",
    "            tables_copy.append(copy.deepcopy(table))\n",
    "        \n",
    "        #calculate hash sum for each entry in each table\n",
    "        for table in tables_copy:\n",
    "            table_attrs = [key for key in table[0].keys() if key in join_attrs]\n",
    "            norm_hashed_table = {}\n",
    "            hashed_table = {}\n",
    "            hashed_table = {attr: minhash._min_hash([attr], table, attrs_hash_dict[attr]) for attr in table_attrs}\n",
    "            for k,v in hashed_table.items():\n",
    "                norm_v = {normalize(0, minhash.NEXTPRIME, v1): v2 for v1, v2 in v.items()}\n",
    "                norm_hashed_table[k] = norm_v\n",
    "            for i in range(len(table)):\n",
    "                entry = table[i]\n",
    "                hash_scores = []\n",
    "                for k, v in norm_hashed_table.items():\n",
    "                    for v1, v2 in v.items():\n",
    "                        if i in v2:\n",
    "                            hash_scores.append(v1)\n",
    "                            break\n",
    "                entry[\"hash sum\"] = sum(hash_scores)\n",
    "        \n",
    "        #filter for all entries whose cdf <= sampling probability\n",
    "        filtered_tables = []\n",
    "        start = time.time()\n",
    "        for i in range(len(tables)):\n",
    "            table = tables_copy[i]\n",
    "            filtered_table = []\n",
    "            if \"hash sum\" in table[0].keys():\n",
    "                n_join_attrs = len([i for i in table[0].keys() if i in join_attrs])\n",
    "                for entry in table:\n",
    "                    if cdfsampler.cdf(n_join_attrs, entry[\"hash sum\"]) <= sampling_threshold:\n",
    "                        filtered_table.append(entry)\n",
    "            else:\n",
    "                filtered_table = table\n",
    "            if len(filtered_table) > 0:\n",
    "                filtered_tables.append(filtered_table)\n",
    "            else:\n",
    "                filtered_tables = []\n",
    "                break\n",
    "        filtered_time = time.time() - start        \n",
    "\n",
    "                \n",
    "        #filter for all entries whose cdf <= random probability\n",
    "        random_tables = []\n",
    "        start = time.time()\n",
    "        for i in range(len(tables)):\n",
    "            table = tables_copy[i]\n",
    "            if random_threshold == 0.0:\n",
    "                random_threshold = random.uniform(0,1)\n",
    "            random_table = []\n",
    "            if \"hash sum\" in table[0].keys():\n",
    "                n_join_attrs = len([i for i in table[0].keys() if i in join_attrs])\n",
    "                for entry in table:\n",
    "                    if random_threshold < cdfsampler.cdf(n_join_attrs, entry[\"hash sum\"]):\n",
    "                        random_table.append(entry)\n",
    "            else:\n",
    "                random_table = table\n",
    "            if len(random_table) > 0:\n",
    "                random_tables.append(random_table)\n",
    "            else:\n",
    "                random_tables = []\n",
    "                break\n",
    "        random_time = time.time() - start\n",
    "        \n",
    "        if filtered_tables == []:\n",
    "            joined_filtered_tables = []\n",
    "        else:\n",
    "            filtered_tables = remove_hashsum_key(filtered_tables)\n",
    "            start = time.time()\n",
    "            joined_filtered_tables = natjoin.natural_join(filtered_tables)\n",
    "            filtered_time += time.time() - start\n",
    "            \n",
    "        if random_tables == []:\n",
    "            joined_random_tables = []\n",
    "        else:\n",
    "            random_tables = remove_hashsum_key(random_tables)\n",
    "            start = time.time()\n",
    "            joined_random_tables = natjoin.natural_join(random_tables)\n",
    "            random_time += time.time() - start\n",
    "            \n",
    "\n",
    "        return [(joined_filtered_tables, filtered_time), (joined_random_tables, random_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
